{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###**Task 1 | GPT-2 Model & Checkpoints**"
      ],
      "metadata": {
        "id": "lGzYyVw9_Dvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###This code defines a GPT-2-like transformer model in PyTorch. Here's a breakdown of the code:\n",
        "\n",
        "- **Multi-Head Attention:** The `MultiHeadAttention` class implements the multi-head attention mechanism, which splits the input into multiple heads and computes attention scores between them.\n",
        "\n",
        "- **Feed-Forward Network:** The `FeedForward` class defines a simple feed-forward neural network used within the transformer block.\n",
        "\n",
        "- **Positional Encoding:** The `PositionalEncoding` class generates positional encodings to provide information about the order of tokens in the input sequence.\n",
        "\n",
        "- **Transformer Block:** The `TransformerBlock` class encapsulates a single block of the transformer architecture, consisting of multi-head attention, layer normalization, and feed-forward layers.\n",
        "\n",
        "- **GPT-2 Model:** The `GPT2` class brings together the transformer blocks and other necessary components (embedding, positional encoding, linear layers) to create a GPT-2 model.\n",
        "\n",
        "- **Sample Usage:** It includes a sample usage section where an instance of the `GPT2` model is created, and a sample input tensor is passed through the model to obtain the output.\n",
        "\n",
        "This code provides the structure for a basic GPT-2-like model. To train this model or use it for specific tasks, you'd need to define a training loop, prepare your dataset, specify a loss function, and optimize the model parameters using gradient descent.\n",
        "\n",
        "The `output` tensor represents the model's predictions based on the sample input tensor. Adjusting the `vocab_size` and `seq_length` variables will allow you to tailor the model to your specific use case or dataset.\n",
        "\n",
        "Ref: a)The [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) paper's architecture.\n",
        "b)[Andrej Karpathy's nanogpt](https://github.com/karpathy/nanoGPT) repository"
      ],
      "metadata": {
        "id": "H1Wp4Mzz__Hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # Linear transformation for query, key, and value\n",
        "        Q = self.query(query)\n",
        "        K = self.key(key)\n",
        "        V = self.value(value)\n",
        "\n",
        "        # Reshape Q, K, and V\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float('-1e20'))  # Masked positions get large negative energy\n",
        "\n",
        "        attention = torch.nn.functional.softmax(energy, dim=-1)\n",
        "        x = torch.matmul(attention, V)\n",
        "\n",
        "        # Reshape and concatenate attention heads\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # Linear transformation for output\n",
        "        x = self.fc_out(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Define Feed-Forward Network\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# Define Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "# Define Transformer Block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, hidden_dim):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, hidden_dim)\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attended = self.attention(x, x, x, mask)\n",
        "        x = self.layer_norm1(x + attended)\n",
        "        feed_forward_output = self.feed_forward(x)\n",
        "        x = self.layer_norm2(x + feed_forward_output)\n",
        "        return x\n",
        "\n",
        "# Define GPT-2 Model\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=768, num_heads=12, num_layers=12, hidden_dim=3072, max_len=512):\n",
        "        super(GPT2, self).__init__()\n",
        "        self.token_embeddings = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.transformer_blocks = nn.ModuleList([TransformerBlock(d_model, num_heads, hidden_dim) for _ in range(num_layers)])\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        token_embed = self.token_embeddings(x)\n",
        "        token_embed = self.positional_encoding(token_embed)\n",
        "\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            token_embed = transformer_block(token_embed, mask)\n",
        "\n",
        "        output = self.fc(token_embed)\n",
        "        return output\n",
        "\n",
        "# Sample usage and testing\n",
        "vocab_size = 10000  # Replace with your actual vocabulary size\n",
        "seq_length = 50  # Replace with the desired sequence length\n",
        "\n",
        "model = GPT2(vocab_size)\n",
        "\n",
        "# Create an example input tensor (batch size of 1 for simplicity)\n",
        "sample_input = torch.randint(0, vocab_size, (1, seq_length))\n",
        "\n",
        "# Perform a forward pass through the model\n",
        "output = model(sample_input)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(\"Output shape:\", output.shape)\n",
        "\n",
        "print(output)\n",
        ""
      ],
      "metadata": {
        "id": "VLt3dN9EgnCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "625cb487-5293-4a78-f90c-5ca15297717b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 50, 10000])\n",
            "tensor([[[ 0.9271, -0.3426,  0.5934,  ..., -0.1170, -1.0391, -0.2518],\n",
            "         [ 0.4209, -1.0054,  1.1158,  ..., -0.3033, -1.0238, -0.5546],\n",
            "         [ 0.2445, -0.9566,  1.4994,  ..., -0.3522, -0.3304, -0.4663],\n",
            "         ...,\n",
            "         [ 0.4212, -0.2445,  0.4920,  ..., -0.3525, -0.1931, -0.8938],\n",
            "         [-0.0057,  0.2506,  0.8035,  ..., -0.8767, -0.3137, -1.0402],\n",
            "         [ 0.1285, -1.0617, -0.0357,  ..., -0.4016, -0.7944, -0.5287]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Task 2 | Transformer Architectural Changes**"
      ],
      "metadata": {
        "id": "qPMXkyz7AfZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Rotary Positional Embedding:\n",
        "\n",
        "This RotaryPositionalEmbedding class is an implementation of Rotary Positional Embedding, which is an alternative to the standard positional encoding in transformer-based models.To explore more about Rotary Positional Embeddings, you can refer to the following paper: [Rotary Position Embeddings](https://arxiv.org/pdf/2104.09864.pdf).\n"
      ],
      "metadata": {
        "id": "y4u62t6YApqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super(RotaryPositionalEmbedding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.init_rotary_embeddings()\n",
        "\n",
        "    def init_rotary_embeddings(self):\n",
        "        # Initialize rotary positional embeddings\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.d_model, 2.0) / self.d_model))\n",
        "        position = torch.arange(0, self.max_len).unsqueeze(1)\n",
        "        sin_pos = torch.sin(position * inv_freq)\n",
        "        cos_pos = torch.cos(position * inv_freq)\n",
        "\n",
        "        self.pos_embedding_sin = nn.Parameter(sin_pos)\n",
        "        self.pos_embedding_cos = nn.Parameter(cos_pos)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply Rotary Positional Embedding to the input sequence x\n",
        "        seq_len = x.size(1)\n",
        "        pos_embedding = torch.cat([self.pos_embedding_sin[:seq_len, :], self.pos_embedding_cos[:seq_len, :]], dim=-1)\n",
        "        return x + pos_embedding\n"
      ],
      "metadata": {
        "id": "RM4_pD2-sLVQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Group Query Attention:\n",
        "\n",
        "This code defines a GroupQueryAttention module, which is an altered version of the Multi-Head Attention mechanism. This attention mechanism divides the input into multiple groups and performs attention separately within each group.\n",
        "\n",
        "Ref: The Group Query Attention mechanism following the insights from the [Ainslie et. al. GQA: Training Generalized Multi-Query Transforme](https://arxiv.org/pdf/2305.13245v2.pdf)"
      ],
      "metadata": {
        "id": "-KeO1bKXBfp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupQueryAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, num_groups=4):\n",
        "        super(GroupQueryAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.num_groups = num_groups\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        assert d_model % num_groups == 0, \"d_model must be divisible by num_groups\"\n",
        "        assert num_heads % num_groups == 0, \"num_heads must be divisible by num_groups\"\n",
        "\n",
        "        self.group_dim = d_model // num_groups\n",
        "        self.group_heads = num_heads // num_groups\n",
        "\n",
        "        # Linear transformations for queries, keys, and values for each group\n",
        "        self.query = nn.Linear(self.group_dim, self.group_heads * self.head_dim)\n",
        "        self.key = nn.Linear(self.group_dim, self.group_heads * self.head_dim)\n",
        "        self.value = nn.Linear(self.group_dim, self.group_heads * self.head_dim)\n",
        "\n",
        "        # Output projection\n",
        "        self.fc_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # Reshape queries, keys, and values into groups\n",
        "        query = query.view(batch_size, -1, self.num_groups, self.group_dim).permute(0, 2, 1, 3)\n",
        "        key = key.view(batch_size, -1, self.num_groups, self.group_dim).permute(0, 2, 1, 3)\n",
        "        value = value.view(batch_size, -1, self.num_groups, self.group_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Linear transformations\n",
        "        Q = self.query(query)\n",
        "        K = self.key(key)\n",
        "        V = self.value(value)\n",
        "\n",
        "        # Reshape Q, K, V for attention calculation\n",
        "        Q = Q.view(batch_size, self.num_groups, -1, self.head_dim).permute(0, 1, 3, 2)\n",
        "        K = K.view(batch_size, self.num_groups, -1, self.head_dim).permute(0, 1, 3, 2)\n",
        "        V = V.view(batch_size, self.num_groups, -1, self.head_dim).permute(0, 1, 3, 2)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float('-1e20'))  # Masked positions get large negative energy\n",
        "\n",
        "        attention = torch.nn.functional.softmax(energy, dim=-1)\n",
        "        x = torch.matmul(attention, V)\n",
        "\n",
        "        # Reshape and concatenate attention heads\n",
        "        x = x.permute(0, 1, 3, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # Linear transformation for output\n",
        "        x = self.fc_out(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "NVWused9iCwX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Transformer Block with Group Query Attention and Rotary Positional Embedding\n",
        "class TransformerBlock_Groupquery_RoPE(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, hidden_dim):\n",
        "        super(TransformerBlock_Groupquery_RoPE, self).__init__()\n",
        "        self.attention = GroupQueryAttention(d_model, num_heads)  # Group Query Attention\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, d_model)\n",
        "        )\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.rotary_pos_embedding = RotaryPositionalEmbedding(d_model)  # Rotary Positional Embedding\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attended = self.attention(x, x, x, mask)\n",
        "        x = self.layer_norm1(x + attended)\n",
        "        x = self.rotary_pos_embedding(x)  # Applying Rotary Positional Embedding\n",
        "        feed_forward_output = self.feed_forward(x)\n",
        "        x = self.layer_norm2(x + feed_forward_output)\n",
        "        return x\n",
        "\n",
        "# Define Modified GPT-2 model with Rotary Positional Embedding and Group Query Attention\n",
        "class ModifiedGPT2(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=768, num_heads=12, num_layers=12, hidden_dim=3072):\n",
        "        super(ModifiedGPT2, self).__init__()\n",
        "        self.token_embeddings = nn.Embedding(vocab_size, d_model)\n",
        "        self.transformer_blocks = nn.ModuleList([TransformerBlock_Groupquery_RoPE(d_model, num_heads, hidden_dim) for _ in range(num_layers)])\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        token_embed = self.token_embeddings(x)\n",
        "\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            token_embed = transformer_block(token_embed, mask)\n",
        "\n",
        "        output = self.fc(token_embed)\n",
        "        return output\n",
        "\n",
        "# Sample usage and testing\n",
        "vocab_size = 10000  # Replace with your actual vocabulary size\n",
        "seq_length = 50  # Replace with the desired sequence length\n",
        "\n",
        "model = ModifiedGPT2(vocab_size)\n",
        "\n",
        "# Create an example input tensor (batch size of 1 for simplicity)\n",
        "sample_input = torch.randint(0, vocab_size, (1, seq_length))\n",
        "\n",
        "# Perform a forward pass through the model\n",
        "output = model(sample_input)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(\"Output shape:\", output.shape)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2Kd5M6SsEG0",
        "outputId": "8739f339-80af-471c-8f4b-261edcbd013f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 50, 10000])\n",
            "tensor([[[-0.4557, -0.2189,  0.4474,  ...,  1.4816, -0.6794,  1.3982],\n",
            "         [-0.3831,  0.0115,  0.5123,  ...,  1.3253, -0.6766,  1.3296],\n",
            "         [-0.3300,  0.1373,  0.7185,  ...,  1.2620, -0.7739,  1.3185],\n",
            "         ...,\n",
            "         [-0.4795, -0.1452,  1.1642,  ...,  0.8815,  0.3852, -0.1648],\n",
            "         [-0.2475, -0.3346,  1.1498,  ...,  1.0655,  0.1698, -0.2334],\n",
            "         [ 0.0063, -0.3363,  1.0891,  ...,  1.3330, -0.1069, -0.2190]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sliding Window Attention:\n",
        "\n",
        "\n",
        "The SlidingWindowAttention module implements a variation of the self-attention mechanism where attention is applied within a sliding window across the input sequence. This is useful when dealing with long sequences as it reduces computational complexity compared to standard self-attention while maintaining some degree of local attention.\n",
        "\n",
        "This mechanism divides the input sequence into overlapping windows and computes attention separately for each window, reducing the computational complexity while retaining some local context information.\n",
        "\n",
        "Ref: The work by [Beltagy et. al. Longformer](https://arxiv.org/pdf/2004.05150v2.pdf) for better comprehension of its implementation and advantages."
      ],
      "metadata": {
        "id": "9emzRJuzCDam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SlidingWindowAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, window_size):\n",
        "        super(SlidingWindowAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        Q = self.query(query)\n",
        "        K = self.key(key)\n",
        "        V = self.value(value)\n",
        "\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.d_model // self.num_heads).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.d_model // self.num_heads).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.d_model // self.num_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "        window_size = min(self.window_size, K.size(2))\n",
        "\n",
        "        # Implement sliding window attention\n",
        "        output_chunks = []\n",
        "        for i in range(0, K.size(2), window_size):\n",
        "            k_chunk = K[:, :, i:i+window_size]  # Split keys into chunks\n",
        "            v_chunk = V[:, :, i:i+window_size]  # Split values into chunks\n",
        "\n",
        "            energy = torch.matmul(Q, k_chunk.permute(0, 1, 3, 2)) / (self.d_model ** 0.5)\n",
        "            if mask is not None:\n",
        "                energy = energy.masked_fill(mask == 0, float('-1e20'))\n",
        "\n",
        "            attention = torch.nn.functional.softmax(energy, dim=-1)\n",
        "            output_chunk = torch.matmul(attention, v_chunk)\n",
        "            output_chunks.append(output_chunk)\n",
        "\n",
        "        output = torch.cat(output_chunks, dim=2)\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.fc_out(output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "gTpRvGiVsVrV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Transformer Block with Group Query Attention, Rotary Positional Embedding, and Sliding Window Attention\n",
        "class TransformerBlock_Groupquery_RoPE_SWA(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, hidden_dim, window_size):\n",
        "        super(TransformerBlock_Groupquery_RoPE_SWA, self).__init__()\n",
        "        self.attention = SlidingWindowAttention(d_model, num_heads, window_size)  # Sliding Window Attention\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, d_model)\n",
        "        )\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.rotary_pos_embedding = RotaryPositionalEmbedding(d_model)  # Rotary Positional Embedding\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attended = self.attention(x, x, x, mask)\n",
        "        x = self.layer_norm1(x + attended)\n",
        "        x = self.rotary_pos_embedding(x)  # Applying Rotary Positional Embedding\n",
        "        feed_forward_output = self.feed_forward(x)\n",
        "        x = self.layer_norm2(x + feed_forward_output)\n",
        "        return x\n",
        "\n",
        "# Define Modified GPT-2 model with Rotary Positional Embedding, Group Query Attention, and Sliding Window Attention\n",
        "class ModifiedGPT2_SWA(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=768, num_heads=12, num_layers=12, hidden_dim=3072, window_size=128):\n",
        "        super(ModifiedGPT2_SWA, self).__init__()\n",
        "        self.token_embeddings = nn.Embedding(vocab_size, d_model)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock_Groupquery_RoPE_SWA(d_model, num_heads, hidden_dim, window_size) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        token_embed = self.token_embeddings(x)\n",
        "\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            token_embed = transformer_block(token_embed, mask)\n",
        "\n",
        "        output = self.fc(token_embed)\n",
        "        return output\n",
        "\n",
        "# Sample usage and testing\n",
        "vocab_size = 10000  # Replace with your actual vocabulary size\n",
        "seq_length = 50  # Replace with the desired sequence length\n",
        "\n",
        "model3 = ModifiedGPT2_SWA(vocab_size)\n",
        "\n",
        "# Create an example input tensor (batch size of 1 for simplicity)\n",
        "sample_input = torch.randint(0, vocab_size, (1, seq_length))\n",
        "\n",
        "# Perform a forward pass through the model\n",
        "output = model3(sample_input)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzJWOTDoutdQ",
        "outputId": "128f426a-8aee-4eb1-c6c3-4a5c9d01cf9d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 50, 10000])\n",
            "tensor([[[ 1.9934,  0.2343,  0.5987,  ...,  1.4065,  1.1938, -0.1305],\n",
            "         [ 2.1030,  0.2031,  0.5219,  ...,  1.4025,  1.3405, -0.3348],\n",
            "         [ 1.9928,  0.2200,  0.6767,  ...,  1.3596,  1.4793, -0.7478],\n",
            "         ...,\n",
            "         [ 0.3951, -0.2136,  0.5191,  ..., -0.1982,  1.0205,  0.0118],\n",
            "         [ 0.5461, -0.0633,  0.3845,  ...,  0.0713,  1.0498,  0.0813],\n",
            "         [ 0.7763,  0.0673,  0.3135,  ...,  0.3663,  1.0490, -0.0674]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Task 3: Training Loop Implementation**"
      ],
      "metadata": {
        "id": "sEJLORPH7GVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.Single GPU Training Loop:"
      ],
      "metadata": {
        "id": "6BjUxjVH7kbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the GPT-2 model\n",
        "vocab_size = 10000  # Replace with the actual vocabulary size\n",
        "model = GPT2SingleGPU(vocab_size)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "train_dataset = ...  # Your training dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # Set your desired number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{batch_idx+1}/{len(train_loader)}] Loss: {total_loss / 100:.4f}\")\n",
        "            total_loss = 0.0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F8PCV5-JvoSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Distributed Data Parallel (DDP):\n",
        "\n",
        "DistributedDataParallel (DDP) implements data parallelism at the module level which can run across multiple machines. Applications using DDP should spawn multiple processes and create a single DDP instance per process. DDP uses collective communications in the torch.distributed package to synchronize gradients and buffers. More specifically, DDP registers an autograd hook for each parameter given by model.parameters() and the hook will fire when the corresponding gradient is computed in the backward pass.\n",
        "\n",
        "Ref: [PyTorch's DDP tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)"
      ],
      "metadata": {
        "id": "ktCdTBQ_8lU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "\n",
        "# Function to train the model\n",
        "def train_DDP(rank, world_size):\n",
        "    torch.manual_seed(1234)\n",
        "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
        "    torch.cuda.set_device(rank)\n",
        "\n",
        "    # Define your dataset and DataLoader\n",
        "    train_dataset = ...  # Your training dataset\n",
        "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
        "\n",
        "    # Create an instance of the GPT-2 model\n",
        "    vocab_size = 10000  # Replace with the actual vocabulary size\n",
        "    model = GPT2(vocab_size)\n",
        "    model.to(rank)\n",
        "    model = DDP(model, device_ids=[rank])\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "     # Training loop\n",
        "    num_epochs = 10  # Set your desired number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(rank), targets.to(rank)\n",
        "            optimizer.zero_grad()\n",
        "            ......\n",
        "            .....\n",
        "\n"
      ],
      "metadata": {
        "id": "FwFpp2Yt7rqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Fully Sharded Data Parallel (FSDP)\n",
        "\n",
        "Ref:  [Gupta et al., 2020, Training GPT-3 Like Models on a Single Machine](https://arxiv.org/pdf/2101.06840.pdf)"
      ],
      "metadata": {
        "id": "B1sktUid9Z-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
        "from torch.distributed.fsdp import ShardingStrategy\n",
        "\n",
        "\n",
        "# Define the FSDP model and optimizer\n",
        "fsdp_model = FullyShardedDataParallel(model)  # Create an FSDP-wrapped model\n",
        "optimizer = torch.optim.Adam(fsdp_model.parameters())\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for data in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_data, target = data\n",
        "\n",
        "        # Forward pass\n",
        "        output = fsdp_model(input_data)\n",
        "        loss = loss_function(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient reduction across shards\n",
        "        fsdp_model.reduce()\n",
        "\n",
        "        # Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "    # Optional: Synchronize optimizer state\n",
        "    fsdp_model.sync_optimizer_state()\n"
      ],
      "metadata": {
        "id": "C0evEc0G9lcQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
